{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42fcea9d-43f9-44af-a956-5c1bdd6c3eb4",
   "metadata": {},
   "source": [
    "# Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373acfe1-5257-41d7-9665-ecce4340d991",
   "metadata": {},
   "source": [
    " tlc data\n",
    "### feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8c16013-ef75-4c16-8a3c-7fbabf3f457e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark.sql.functions as F\n",
    "import warnings\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col, unix_timestamp, date_format, when, weekofyear, desc, round, expr, hour\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7265d3d-942a-47ed-bb78-9764a321c971",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/20 17:59:00 WARN Utils: Your hostname, LAPTOP-D9335T9D resolves to a loopback address: 127.0.1.1; using 192.168.0.77 instead (on interface wifi0)\n",
      "23/08/20 17:59:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/20 17:59:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 \")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46b9c07-812e-4dcf-8328-2a8a37198aa6",
   "metadata": {},
   "source": [
    "## Preprocess the taxi data and save into raw data file \n",
    "\n",
    "### Removing unneccessary features of Taxi data \n",
    "### saving the new data in the raw data file "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a0022c-00c8-4370-b17d-8e3c2d4ff1be",
   "metadata": {},
   "source": [
    "#### Exmaine the feature Congestion_Surcharge(CS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98eae203-983d-4d94-a7cd-ff2b0b21c155",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of non empty records: %d 14\n",
      "total number of record: %d  53930466\n",
      "2.595935291936843e-07\n"
     ]
    }
   ],
   "source": [
    "count_CS = 0\n",
    "total = 0\n",
    "# interate through each taxi data monthly\n",
    "for i in ['01','02','03','04','05','06']:\n",
    "    taxi_sdf = spark.read.parquet('../data/landing/tlc_data/'+'yellow_tripdata_2018-'+i+'.parquet')\n",
    "    # select the congestion feature\n",
    "    surcharge_data = taxi_sdf.select(\"Congestion_Surcharge\")\n",
    "    # filter out null values\n",
    "    sdf_without_nulls = surcharge_data.na.drop()\n",
    "    #counting \n",
    "    count_CS += sdf_without_nulls.count()\n",
    "    total += taxi_sdf.count()\n",
    "\n",
    "\n",
    "print('number of non empty records: %d', count_CS)\n",
    "print('total number of record: %d ', total)\n",
    "print(count_CS/total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926c8f16-cf0a-43c5-8470-98a8ca4a506b",
   "metadata": {},
   "source": [
    "Since only 14 instance of data with non empty Congestion surcharge. We will not consider in analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bb725e-7c0f-4f87-a2bc-8692e33fb46a",
   "metadata": {},
   "source": [
    "#### Examine the MTA_tax \n",
    "\n",
    "By TLC dictionary, MTA_tax should only be $0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b6913d1-874b-468c-b26c-558292b33805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>MTA_tax</th><th>count(MTA_tax)</th></tr>\n",
       "<tr><td>45.49</td><td>1</td></tr>\n",
       "<tr><td>0.0</td><td>35938</td></tr>\n",
       "<tr><td>23.8</td><td>1</td></tr>\n",
       "<tr><td>0.6</td><td>1</td></tr>\n",
       "<tr><td>0.35</td><td>4</td></tr>\n",
       "<tr><td>0.5</td><td>8720505</td></tr>\n",
       "<tr><td>0.32</td><td>1</td></tr>\n",
       "<tr><td>0.4</td><td>1</td></tr>\n",
       "<tr><td>3.0</td><td>73</td></tr>\n",
       "<tr><td>6.33</td><td>1</td></tr>\n",
       "<tr><td>10.41</td><td>1</td></tr>\n",
       "<tr><td>-0.5</td><td>4153</td></tr>\n",
       "<tr><td>0.9</td><td>1</td></tr>\n",
       "<tr><td>0.01</td><td>5</td></tr>\n",
       "<tr><td>15.0</td><td>1</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------+--------------+\n",
       "|MTA_tax|count(MTA_tax)|\n",
       "+-------+--------------+\n",
       "|  45.49|             1|\n",
       "|    0.0|         35938|\n",
       "|   23.8|             1|\n",
       "|    0.6|             1|\n",
       "|   0.35|             4|\n",
       "|    0.5|       8720505|\n",
       "|   0.32|             1|\n",
       "|    0.4|             1|\n",
       "|    3.0|            73|\n",
       "|   6.33|             1|\n",
       "|  10.41|             1|\n",
       "|   -0.5|          4153|\n",
       "|    0.9|             1|\n",
       "|   0.01|             5|\n",
       "|   15.0|             1|\n",
       "+-------+--------------+"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we randomly examine yellow taxi data in Jan\n",
    "taxi_sdf = spark.read.parquet('../data/landing/tlc_data/'+'yellow_tripdata_2018-01.parquet')\n",
    "df = taxi_sdf.select(\"MTA_tax\")\n",
    "counts = df[['MTA_tax']] \\\n",
    "                .groupby('MTA_tax') \\\n",
    "                .agg(\n",
    "                    {\n",
    "                        'MTA_tax': 'count' # count number of instances from sample\n",
    "                    }\n",
    "                )\n",
    "\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccdf1db-322a-4a61-b7ad-9fdcdff6ea96",
   "metadata": {},
   "source": [
    "As we can see there are some outlier within the data, we will ensure clean these invalid data before analysis.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5125db7-86f6-4e0b-aa18-a902e6526999",
   "metadata": {},
   "source": [
    "#### Examine Airport_fee attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "325e63e1-b5bb-4612-a57d-5b760546ec94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "53930466\n",
      "2.225087393088723e-07\n"
     ]
    }
   ],
   "source": [
    "count_not_na = 0\n",
    "total = 0\n",
    "for i in ['01','02','03','04','05','06']:\n",
    "    taxi_sdf = spark.read.parquet('../data/landing/tlc_data/'+'yellow_tripdata_2018-'+i+'.parquet')\n",
    "    sdf = taxi_sdf.select(\"Airport_fee\")\n",
    "    sdf_without_nulls = sdf.na.drop()\n",
    "    count_not_na += sdf_without_nulls.count()\n",
    "    total += taxi_sdf.count()\n",
    "\n",
    "print(count_not_na)\n",
    "print(total)\n",
    "print(count_not_na/total)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67804a65-cd99-458d-ba24-43042c1b6a21",
   "metadata": {},
   "source": [
    "Similarly, we see only 12 record with non null vaule for airport_fee hence we will not consider this feature in our further investigation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fde05f1-a168-4993-bc04-3380c0ad7f77",
   "metadata": {},
   "source": [
    "#### Examine payment type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d10ce46-9660-4cb4-9152-c100a57a2a08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>payment_type</th><th>count(payment_type)</th></tr>\n",
       "<tr><td>1</td><td>37571710</td></tr>\n",
       "<tr><td>3</td><td>285325</td></tr>\n",
       "<tr><td>2</td><td>15996221</td></tr>\n",
       "<tr><td>4</td><td>77210</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------------+-------------------+\n",
       "|payment_type|count(payment_type)|\n",
       "+------------+-------------------+\n",
       "|           1|           37571710|\n",
       "|           3|             285325|\n",
       "|           2|           15996221|\n",
       "|           4|              77210|\n",
       "+------------+-------------------+"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_sdf = spark.read.parquet('../data/landing/tlc_data/*')\n",
    "df = taxi_sdf.select(\"payment_type\")\n",
    "counts = df[['payment_type']] \\\n",
    "                .groupby('payment_type') \\\n",
    "                .agg(\n",
    "                    {\n",
    "                        'payment_type': 'count' # count number of instances from sample\n",
    "                    }\n",
    "                )\n",
    "\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48193df7-b1c2-408d-b2be-1ae428a3a18a",
   "metadata": {},
   "source": [
    "Similarly, we see only 12 record with non null vaule for airport_fee hence we will not consider this feature in our further investigation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368cda6e-492f-4fa8-b9f9-024bac160a73",
   "metadata": {},
   "source": [
    "## Filter the original tlc taxi data and save to raw data file\n",
    "Ensure consistent data type for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4127a536-3c4d-4392-a06e-3055c3166271",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# saving the taxi data with only relevant features \n",
    "\n",
    "# Define the data type conversions for each column\n",
    "data_type_mappings = {\n",
    "    'tpep_pickup_datetime': 'timestamp',\n",
    "    'tpep_dropoff_datetime': 'timestamp',\n",
    "    'passenger_count': 'int',\n",
    "    'trip_distance': 'double',\n",
    "    'PULocationID': 'int',\n",
    "    'DOLocationID': 'int',\n",
    "    'payment_type': 'int',\n",
    "    'tip_amount': 'double',\n",
    "    'fare_amount': 'double'\n",
    "}\n",
    "\n",
    "\n",
    "for i in ['01','02','03','04','05','06']:\n",
    "    taxi_sdf = spark.read.parquet('../data/landing/tlc_data/'+'yellow_tripdata_2018-'+i+'.parquet')\n",
    "\n",
    "    # since we will not include features like VendorID, Extra and MTA_tax. \n",
    "    # we will remove any data that breaks the bussiness rule for these features\n",
    "    # such we will remove any instance with invalid vendorid, and remove any instance with mta_tax!= 0.5\n",
    "    taxi_sdf = taxi_sdf.filter(taxi_sdf[\"VendorID\"].isin([1,2]))\n",
    "    taxi_sdf = taxi_sdf.filter(F.col('MTA_tax') == 0.5)\n",
    "\n",
    "    # remove any data with invalid Extra any extra other than 0.5 and 1 or 0\n",
    "    taxi_sdf = taxi_sdf.filter(taxi_sdf[\"Extra\"].isin([0.5,1,0]))\n",
    "\n",
    "    # remove any data with invalid RateCodeID\n",
    "    taxi_sdf = taxi_sdf.filter(taxi_sdf[\"RateCodeID\"].isin([1,2,3,4,5,6]))\n",
    "\n",
    "    # retain any record with fare amount less than total amount\n",
    "    taxi_sdf = taxi_sdf.filter(taxi_sdf[\"Fare_amount\"] < taxi_sdf[\"Total_amount\"])\n",
    "\n",
    "    # retain record with valid toll_amount\n",
    "    taxi_sdf = taxi_sdf.filter(F.col('Tolls_amount') >= 0)\n",
    "    taxi_sdf= taxi_sdf.filter(F.col('Tolls_amount') <=38)\n",
    "\n",
    " \n",
    "    filter_sdf = taxi_sdf.select('tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', \n",
    "                             'PULocationID', 'DOLocationID', 'payment_type', 'tip_amount', 'Fare_amount')\n",
    "\n",
    "\n",
    "\n",
    "    for column_name, new_data_type in data_type_mappings.items():\n",
    "         filter_sdf = filter_sdf.withColumn(column_name, col(column_name).cast(new_data_type))\n",
    "\n",
    "    consistent_col_casing = [F.col(col_name).alias(col_name.lower()) for col_name in filter_sdf.columns]\n",
    "    filter_sdf = filter_sdf.select(*consistent_col_casing)\n",
    "\n",
    "    \n",
    "\n",
    "    output_folder = \"../data/raw/taxi_data/\" + i  # Replace with the desired output folder\n",
    "    filter_sdf.write.parquet(output_folder, mode=\"overwrite\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3b0652-2b1f-41ea-a936-128b33472397",
   "metadata": {},
   "source": [
    "#### Quick exmaine the output in raw data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d18ce85-20ea-4ae5-b322-5ac07aaf41d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_sdf = spark.read.parquet('../data/raw/taxi_data/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1bc3452-bca2-4817-8b14-9fbd7405031d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>tpep_pickup_datetime</th><th>tpep_dropoff_datetime</th><th>passenger_count</th><th>trip_distance</th><th>pulocationid</th><th>dolocationid</th><th>payment_type</th><th>tip_amount</th><th>fare_amount</th></tr>\n",
       "<tr><td>2018-01-01 00:21:05</td><td>2018-01-01 00:24:23</td><td>1</td><td>0.5</td><td>41</td><td>24</td><td>2</td><td>0.0</td><td>4.5</td></tr>\n",
       "<tr><td>2018-01-01 00:44:55</td><td>2018-01-01 01:03:05</td><td>1</td><td>2.7</td><td>239</td><td>140</td><td>2</td><td>0.0</td><td>14.0</td></tr>\n",
       "<tr><td>2018-01-01 00:08:26</td><td>2018-01-01 00:14:21</td><td>2</td><td>0.8</td><td>262</td><td>141</td><td>1</td><td>1.0</td><td>6.0</td></tr>\n",
       "<tr><td>2018-01-01 00:20:22</td><td>2018-01-01 00:52:51</td><td>1</td><td>10.2</td><td>140</td><td>257</td><td>2</td><td>0.0</td><td>33.5</td></tr>\n",
       "<tr><td>2018-01-01 00:09:18</td><td>2018-01-01 00:27:06</td><td>2</td><td>2.5</td><td>246</td><td>239</td><td>1</td><td>2.75</td><td>12.5</td></tr>\n",
       "<tr><td>2018-01-01 00:29:29</td><td>2018-01-01 00:32:48</td><td>3</td><td>0.5</td><td>143</td><td>143</td><td>2</td><td>0.0</td><td>4.5</td></tr>\n",
       "<tr><td>2018-01-01 00:38:08</td><td>2018-01-01 00:48:24</td><td>2</td><td>1.7</td><td>50</td><td>239</td><td>1</td><td>2.05</td><td>9.0</td></tr>\n",
       "<tr><td>2018-01-01 00:49:29</td><td>2018-01-01 00:51:53</td><td>1</td><td>0.7</td><td>239</td><td>238</td><td>1</td><td>1.0</td><td>4.0</td></tr>\n",
       "<tr><td>2018-01-01 00:56:38</td><td>2018-01-01 01:01:05</td><td>1</td><td>1.0</td><td>238</td><td>24</td><td>1</td><td>1.7</td><td>5.5</td></tr>\n",
       "<tr><td>2018-01-01 00:17:04</td><td>2018-01-01 00:22:24</td><td>1</td><td>0.7</td><td>170</td><td>170</td><td>2</td><td>0.0</td><td>5.5</td></tr>\n",
       "<tr><td>2018-01-01 00:41:03</td><td>2018-01-01 00:46:49</td><td>1</td><td>0.6</td><td>162</td><td>229</td><td>1</td><td>1.35</td><td>5.5</td></tr>\n",
       "<tr><td>2018-01-01 00:52:54</td><td>2018-01-01 01:17:33</td><td>1</td><td>3.5</td><td>141</td><td>113</td><td>2</td><td>0.0</td><td>16.5</td></tr>\n",
       "<tr><td>2018-01-01 00:17:54</td><td>2018-01-01 00:22:05</td><td>1</td><td>1.04</td><td>137</td><td>224</td><td>2</td><td>0.0</td><td>5.5</td></tr>\n",
       "<tr><td>2018-01-01 00:24:47</td><td>2018-01-01 00:34:20</td><td>1</td><td>1.22</td><td>224</td><td>79</td><td>2</td><td>0.0</td><td>7.5</td></tr>\n",
       "<tr><td>2018-01-01 00:37:57</td><td>2018-01-01 00:53:43</td><td>1</td><td>1.92</td><td>234</td><td>100</td><td>2</td><td>0.0</td><td>10.0</td></tr>\n",
       "<tr><td>2018-01-01 00:35:53</td><td>2018-01-01 00:52:59</td><td>1</td><td>5.7</td><td>13</td><td>189</td><td>1</td><td>4.05</td><td>19.0</td></tr>\n",
       "<tr><td>2018-01-01 00:30:47</td><td>2018-01-01 01:13:20</td><td>1</td><td>3.74</td><td>48</td><td>236</td><td>1</td><td>6.7</td><td>25.5</td></tr>\n",
       "<tr><td>2018-01-01 00:21:45</td><td>2018-01-01 00:25:58</td><td>2</td><td>0.6</td><td>163</td><td>162</td><td>1</td><td>1.7</td><td>4.5</td></tr>\n",
       "<tr><td>2018-01-01 00:31:11</td><td>2018-01-01 01:07:56</td><td>1</td><td>10.9</td><td>229</td><td>61</td><td>2</td><td>0.0</td><td>35.0</td></tr>\n",
       "<tr><td>2018-01-01 00:15:42</td><td>2018-01-01 00:21:38</td><td>5</td><td>1.22</td><td>236</td><td>75</td><td>2</td><td>0.0</td><td>6.0</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+--------------------+---------------------+---------------+-------------+------------+------------+------------+----------+-----------+\n",
       "|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|pulocationid|dolocationid|payment_type|tip_amount|fare_amount|\n",
       "+--------------------+---------------------+---------------+-------------+------------+------------+------------+----------+-----------+\n",
       "| 2018-01-01 00:21:05|  2018-01-01 00:24:23|              1|          0.5|          41|          24|           2|       0.0|        4.5|\n",
       "| 2018-01-01 00:44:55|  2018-01-01 01:03:05|              1|          2.7|         239|         140|           2|       0.0|       14.0|\n",
       "| 2018-01-01 00:08:26|  2018-01-01 00:14:21|              2|          0.8|         262|         141|           1|       1.0|        6.0|\n",
       "| 2018-01-01 00:20:22|  2018-01-01 00:52:51|              1|         10.2|         140|         257|           2|       0.0|       33.5|\n",
       "| 2018-01-01 00:09:18|  2018-01-01 00:27:06|              2|          2.5|         246|         239|           1|      2.75|       12.5|\n",
       "| 2018-01-01 00:29:29|  2018-01-01 00:32:48|              3|          0.5|         143|         143|           2|       0.0|        4.5|\n",
       "| 2018-01-01 00:38:08|  2018-01-01 00:48:24|              2|          1.7|          50|         239|           1|      2.05|        9.0|\n",
       "| 2018-01-01 00:49:29|  2018-01-01 00:51:53|              1|          0.7|         239|         238|           1|       1.0|        4.0|\n",
       "| 2018-01-01 00:56:38|  2018-01-01 01:01:05|              1|          1.0|         238|          24|           1|       1.7|        5.5|\n",
       "| 2018-01-01 00:17:04|  2018-01-01 00:22:24|              1|          0.7|         170|         170|           2|       0.0|        5.5|\n",
       "| 2018-01-01 00:41:03|  2018-01-01 00:46:49|              1|          0.6|         162|         229|           1|      1.35|        5.5|\n",
       "| 2018-01-01 00:52:54|  2018-01-01 01:17:33|              1|          3.5|         141|         113|           2|       0.0|       16.5|\n",
       "| 2018-01-01 00:17:54|  2018-01-01 00:22:05|              1|         1.04|         137|         224|           2|       0.0|        5.5|\n",
       "| 2018-01-01 00:24:47|  2018-01-01 00:34:20|              1|         1.22|         224|          79|           2|       0.0|        7.5|\n",
       "| 2018-01-01 00:37:57|  2018-01-01 00:53:43|              1|         1.92|         234|         100|           2|       0.0|       10.0|\n",
       "| 2018-01-01 00:35:53|  2018-01-01 00:52:59|              1|          5.7|          13|         189|           1|      4.05|       19.0|\n",
       "| 2018-01-01 00:30:47|  2018-01-01 01:13:20|              1|         3.74|          48|         236|           1|       6.7|       25.5|\n",
       "| 2018-01-01 00:21:45|  2018-01-01 00:25:58|              2|          0.6|         163|         162|           1|       1.7|        4.5|\n",
       "| 2018-01-01 00:31:11|  2018-01-01 01:07:56|              1|         10.9|         229|          61|           2|       0.0|       35.0|\n",
       "| 2018-01-01 00:15:42|  2018-01-01 00:21:38|              5|         1.22|         236|          75|           2|       0.0|        6.0|\n",
       "+--------------------+---------------------+---------------+-------------+------------+------------+------------+----------+-----------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_sdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f898e39d-b1be-49f4-b0fa-237e4607c654",
   "metadata": {},
   "source": [
    "# Filtering taxi data based on business rules and save in curate data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2b128e-f8e6-489b-8790-1e252ad47519",
   "metadata": {},
   "source": [
    "### Filtering and cleanning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5385fdc2-91bb-4735-b487-71ab37d2e0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Iterate through each month, as total 6 month is too big \n",
    "for i in ['01','02','03','04','05','06']:\n",
    "    temp_sdf = spark.read.parquet('../data/raw/taxi_data/' + i)\n",
    "\n",
    "    # remove any record with invalide passenger counts\n",
    "    # valid passenger count are between 1-7\n",
    "    temp_sdf = temp_sdf.where((F.col('passenger_count') > 0) & (F.col('passenger_count')<=7 ))\n",
    "    \n",
    "    # remove any record with less than 3 dollar fare_amount and more than $120\n",
    "    temp_sdf = temp_sdf.filter(F.col('fare_amount') > 3)\n",
    "    temp_sdf = temp_sdf.filter(F.col('fare_amount') <120)\n",
    "    \n",
    "    \n",
    "    # remove any record with negative tipping and over $100 tip\n",
    "    temp_sdf = temp_sdf.filter(F.col('tip_amount') >= 0)\n",
    "    temp_sdf = temp_sdf.filter(F.col('tip_amount') <= 100)\n",
    "    \n",
    "    #remove any instance with trip distance less than 0.2 mile \n",
    "    temp_sdf = temp_sdf.filter(F.col('trip_distance') >= 0.2)    \n",
    "    \n",
    "    #Define valid payment types\n",
    "    valid_payment_types = [1, 2, 3, 4, 5, 6]\n",
    "    # Remove rows with invalid payment types\n",
    "    temp_sdf = temp_sdf.filter(temp_sdf[\"payment_type\"].isin(valid_payment_types))\n",
    "    \n",
    "    #Define valid location id\n",
    "    location_ids = list(range(1,264))\n",
    "    # Remove rows with invalid pick up location id\n",
    "    # as we are mostly interest in trips that begin inside new york\n",
    "    temp_sdf = temp_sdf.filter(temp_sdf[\"pulocationid\"].isin(location_ids))\n",
    "    \n",
    "    #filter out records where payment_type is not equal to 1 but tip_amount is greater than 0\n",
    "    # as record with payment_type!=1 and tip_amount >0 violates the bussiness rule \n",
    "    ### remove negative tip\n",
    "    temp_sdf = temp_sdf.filter((F.col(\"payment_type\") == 1) | (F.col(\"tip_amount\") <= 0))\n",
    "    \n",
    "    #filter out all record with non credicard payments \n",
    "    temp_sdf = temp_sdf.filter((F.col(\"payment_type\") == 1))\n",
    "    \n",
    "    # Create new feature trip_duration to calculate trip duration in minute\n",
    "    temp_sdf = temp_sdf.withColumn(\"trip_duration_minutes\", round((unix_timestamp(col(\"tpep_dropoff_datetime\")).cast(\"double\") - unix_timestamp(col(\"tpep_pickup_datetime\")).cast(\"double\"))/ 60, 3))\n",
    "    \n",
    "    # remove any trip/instance with trip duration less than a minute and longer than 2.5 hours \n",
    "    temp_sdf = temp_sdf.filter(F.col('trip_duration_minutes') >= 1)\n",
    "    temp_sdf = temp_sdf.filter(F.col('trip_duration_minutes') <= 150)\n",
    "\n",
    "    \n",
    "    # Create new feature of pickup_day and hour (the assitant feature) from pickup_datetime attribute\n",
    "    temp_sdf = temp_sdf.withColumn(\"pickup_day\", date_format(col(\"tpep_pickup_datetime\"), \"yyyy-MM-dd\"))\n",
    "    temp_sdf = temp_sdf.withColumn(\"hour\", date_format(col(\"tpep_pickup_datetime\"), \"HH\").cast(\"int\"))\n",
    "    \n",
    "    #### Remove any record that is not in our research time period \n",
    "    # Define the start and end dates of the desired period\n",
    "    start_date = datetime.strptime(\"2018-01-01\", \"%Y-%m-%d\")\n",
    "    end_date = datetime.strptime(\"2018-06-30\", \"%Y-%m-%d\")\n",
    "    # Filter out records not within the desired period\n",
    "    temp_sdf = temp_sdf.filter((col(\"pickup_day\") >= start_date) & (col(\"pickup_day\") <= end_date))\n",
    "\n",
    "    \n",
    "    # Extract day of the week from pickup_time (Like Monday, Tuesday.....etc)\n",
    "    temp_sdf = temp_sdf.withColumn(\"what_day\", date_format(col(\"pickup_day\"), \"EEEE\"))\n",
    "\n",
    "    # Include new feature week number\n",
    "    temp_sdf = temp_sdf.withColumn(\"week_number\", weekofyear(\"pickup_day\"))\n",
    "    \n",
    "    # Create a new column for weekday or weekend from what day \n",
    "    temp_sdf = temp_sdf.withColumn(\"day_of_the_week\", when(col(\"what_day\").isin(\"Saturday\", \"Sunday\"), \"Weekend\").otherwise(\"Weekday\"))\n",
    "    \n",
    "    \n",
    "    # Drop the tpep_pickup_datetime, tpep_dropoff_datetime, and other temporary/assistant features like hour and what_day, since we no longer using them\n",
    "    temp_sdf = temp_sdf.drop(\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"what_day\",\"payment_type\", \"dolocationid\")\n",
    "    \n",
    "    # dropping any row/instance of data contain missing value\n",
    "    temp_sdf = temp_sdf.na.drop()\n",
    "    \n",
    "    output_folder = \"../data/curated/taxi_data/\" + i  # Replace with the desired output folder\n",
    "    temp_sdf.write.parquet(output_folder, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fd505f7-c396-472f-8386-9d1a3d6f1b5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35960309"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tlc_sdf = spark.read.parquet('../data/curated/taxi_data/*')\n",
    "tlc_sdf.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15f15594-9237-4c16-b68d-bf0bcf2b0bea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>passenger_count</th><th>trip_distance</th><th>pulocationid</th><th>tip_amount</th><th>fare_amount</th><th>trip_duration_minutes</th><th>pickup_day</th><th>hour</th><th>week_number</th><th>day_of_the_week</th></tr>\n",
       "<tr><td>7</td><td>11.79</td><td>186</td><td>0.0</td><td>77.0</td><td>23.783</td><td>2018-06-28</td><td>2</td><td>26</td><td>Weekday</td></tr>\n",
       "<tr><td>7</td><td>17.18</td><td>68</td><td>0.0</td><td>70.0</td><td>29.417</td><td>2018-03-04</td><td>15</td><td>9</td><td>Weekend</td></tr>\n",
       "<tr><td>7</td><td>10.13</td><td>238</td><td>2.0</td><td>70.0</td><td>15.5</td><td>2018-02-10</td><td>2</td><td>6</td><td>Weekend</td></tr>\n",
       "<tr><td>7</td><td>21.16</td><td>138</td><td>17.41</td><td>70.0</td><td>32.7</td><td>2018-03-06</td><td>0</td><td>10</td><td>Weekday</td></tr>\n",
       "<tr><td>7</td><td>16.02</td><td>48</td><td>16.26</td><td>70.0</td><td>24.417</td><td>2018-05-08</td><td>5</td><td>19</td><td>Weekday</td></tr>\n",
       "<tr><td>7</td><td>18.62</td><td>138</td><td>10.0</td><td>70.0</td><td>24.8</td><td>2018-03-14</td><td>1</td><td>11</td><td>Weekday</td></tr>\n",
       "<tr><td>7</td><td>13.47</td><td>48</td><td>16.66</td><td>70.0</td><td>26.817</td><td>2018-02-14</td><td>6</td><td>7</td><td>Weekday</td></tr>\n",
       "<tr><td>7</td><td>0.24</td><td>229</td><td>4.96</td><td>24.0</td><td>4.4</td><td>2018-03-25</td><td>0</td><td>12</td><td>Weekend</td></tr>\n",
       "<tr><td>7</td><td>19.87</td><td>74</td><td>15.16</td><td>75.0</td><td>30.75</td><td>2018-04-06</td><td>22</td><td>14</td><td>Weekday</td></tr>\n",
       "<tr><td>7</td><td>15.73</td><td>138</td><td>0.0</td><td>77.0</td><td>39.967</td><td>2018-02-20</td><td>22</td><td>8</td><td>Weekday</td></tr>\n",
       "<tr><td>7</td><td>2.8</td><td>230</td><td>3.06</td><td>14.0</td><td>18.1</td><td>2018-05-11</td><td>23</td><td>19</td><td>Weekday</td></tr>\n",
       "<tr><td>7</td><td>18.79</td><td>138</td><td>16.51</td><td>76.0</td><td>37.633</td><td>2018-06-03</td><td>18</td><td>22</td><td>Weekend</td></tr>\n",
       "<tr><td>7</td><td>14.59</td><td>107</td><td>0.0</td><td>75.0</td><td>49.0</td><td>2018-05-12</td><td>20</td><td>19</td><td>Weekend</td></tr>\n",
       "<tr><td>7</td><td>0.6</td><td>79</td><td>1.15</td><td>4.5</td><td>3.3</td><td>2018-04-08</td><td>21</td><td>14</td><td>Weekend</td></tr>\n",
       "<tr><td>7</td><td>6.9</td><td>230</td><td>5.0</td><td>70.0</td><td>35.467</td><td>2018-05-16</td><td>22</td><td>20</td><td>Weekday</td></tr>\n",
       "<tr><td>7</td><td>1.3</td><td>79</td><td>1.75</td><td>8.0</td><td>10.05</td><td>2018-01-07</td><td>17</td><td>1</td><td>Weekend</td></tr>\n",
       "<tr><td>7</td><td>6.23</td><td>170</td><td>17.66</td><td>75.0</td><td>36.283</td><td>2018-05-18</td><td>8</td><td>20</td><td>Weekday</td></tr>\n",
       "<tr><td>7</td><td>28.74</td><td>13</td><td>15.31</td><td>70.0</td><td>49.517</td><td>2018-04-22</td><td>18</td><td>16</td><td>Weekend</td></tr>\n",
       "<tr><td>7</td><td>1.3</td><td>166</td><td>2.79</td><td>8.0</td><td>8.583</td><td>2018-05-18</td><td>21</td><td>20</td><td>Weekday</td></tr>\n",
       "<tr><td>7</td><td>8.69</td><td>224</td><td>0.0</td><td>70.0</td><td>26.283</td><td>2018-06-06</td><td>0</td><td>23</td><td>Weekday</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+---------------+-------------+------------+----------+-----------+---------------------+----------+----+-----------+---------------+\n",
       "|passenger_count|trip_distance|pulocationid|tip_amount|fare_amount|trip_duration_minutes|pickup_day|hour|week_number|day_of_the_week|\n",
       "+---------------+-------------+------------+----------+-----------+---------------------+----------+----+-----------+---------------+\n",
       "|              7|        11.79|         186|       0.0|       77.0|               23.783|2018-06-28|   2|         26|        Weekday|\n",
       "|              7|        17.18|          68|       0.0|       70.0|               29.417|2018-03-04|  15|          9|        Weekend|\n",
       "|              7|        10.13|         238|       2.0|       70.0|                 15.5|2018-02-10|   2|          6|        Weekend|\n",
       "|              7|        21.16|         138|     17.41|       70.0|                 32.7|2018-03-06|   0|         10|        Weekday|\n",
       "|              7|        16.02|          48|     16.26|       70.0|               24.417|2018-05-08|   5|         19|        Weekday|\n",
       "|              7|        18.62|         138|      10.0|       70.0|                 24.8|2018-03-14|   1|         11|        Weekday|\n",
       "|              7|        13.47|          48|     16.66|       70.0|               26.817|2018-02-14|   6|          7|        Weekday|\n",
       "|              7|         0.24|         229|      4.96|       24.0|                  4.4|2018-03-25|   0|         12|        Weekend|\n",
       "|              7|        18.79|         138|     16.51|       76.0|               37.633|2018-06-03|  18|         22|        Weekend|\n",
       "|              7|        15.73|         138|       0.0|       77.0|               39.967|2018-02-20|  22|          8|        Weekday|\n",
       "|              7|          2.8|         230|      3.06|       14.0|                 18.1|2018-05-11|  23|         19|        Weekday|\n",
       "|              7|        19.87|          74|     15.16|       75.0|                30.75|2018-04-06|  22|         14|        Weekday|\n",
       "|              7|        14.59|         107|       0.0|       75.0|                 49.0|2018-05-12|  20|         19|        Weekend|\n",
       "|              7|         8.69|         224|       0.0|       70.0|               26.283|2018-06-06|   0|         23|        Weekday|\n",
       "|              7|          6.9|         230|       5.0|       70.0|               35.467|2018-05-16|  22|         20|        Weekday|\n",
       "|              7|          1.3|          79|      1.75|        8.0|                10.05|2018-01-07|  17|          1|        Weekend|\n",
       "|              7|         6.23|         170|     17.66|       75.0|               36.283|2018-05-18|   8|         20|        Weekday|\n",
       "|              7|         3.11|          68|     14.76|       73.0|                8.833|2018-06-14|   1|         24|        Weekday|\n",
       "|              7|          1.3|         166|      2.79|        8.0|                8.583|2018-05-18|  21|         20|        Weekday|\n",
       "|              7|          0.6|          79|      1.15|        4.5|                  3.3|2018-04-08|  21|         14|        Weekend|\n",
       "+---------------+-------------+------------+----------+-----------+---------------------+----------+----+-----------+---------------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tlc_sdf.orderBy(desc(\"passenger_count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565cb532-304a-4cf4-927b-2ca23bab755d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
