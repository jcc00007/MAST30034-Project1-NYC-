{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62e21f7b-f9cb-43c2-8c9e-97036e798f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, unix_timestamp, date_format, when, year, weekofyear, hour, round, expr, log\n",
    "import pyspark.sql.functions as F\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88f51a0d-3e88-4af5-8bf9-1a8b86c6db95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/20 17:55:49 WARN Utils: Your hostname, LAPTOP-D9335T9D resolves to a loopback address: 127.0.1.1; using 192.168.0.77 instead (on interface wifi0)\n",
      "23/08/20 17:55:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/20 17:55:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 \")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64db2c94-e4e5-4878-8d40-536ded7d5a95",
   "metadata": {},
   "source": [
    "## Preprocess the 2019's march tlc taxi data in the same way as we preprocessed training data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1e358b-f3e7-45ca-83ba-80182d7d4311",
   "metadata": {},
   "source": [
    "import the location data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b50bae36-ff81-4137-95f8-369cc961e0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../data/curated/taxi_zone_gdf/taxi_zone_gdf.geojson\"\n",
    "# Read the GeoPandas DataFrame from the specified file\n",
    "gdf = gpd.read_file(file_path)\n",
    "location = spark.createDataFrame(gdf[[\"LocationID\", \"Borough\"]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b668c0e0-745b-421d-afdf-9b5a7286ea9d",
   "metadata": {},
   "source": [
    "### simple type transformation and filtering to raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c2c11e2-ca14-4491-935c-b097d7acfaba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# saving the taxi data with only relevant features \n",
    "\n",
    "# Define the data type conversions for each column\n",
    "data_type_mappings = {\n",
    "'tpep_pickup_datetime': 'timestamp',\n",
    "'tpep_dropoff_datetime': 'timestamp',\n",
    "'passenger_count': 'int',\n",
    "'trip_distance': 'double',\n",
    "'PULocationID': 'int',\n",
    "'DOLocationID': 'int',\n",
    "'payment_type': 'int',\n",
    "'tip_amount': 'double',\n",
    "'fare_amount': 'double'\n",
    "}\n",
    "\n",
    "\n",
    "taxi_sdf = spark.read.parquet('../data/landing/test_taxi_2019/*')\n",
    "\n",
    "# since we will not include features like VendorID, Extra and MTA_tax. \n",
    "# we will remove any data that breaks the bussiness rule for these features\n",
    "# such we will remove any instance with invalid vendorid, and remove any instance with mta_tax!= 0.5\n",
    "taxi_sdf = taxi_sdf.filter(taxi_sdf[\"VendorID\"].isin([1,2]))\n",
    "taxi_sdf = taxi_sdf.filter(F.col('MTA_tax') == 0.5)\n",
    "\n",
    "# remove any data with invalid Extra any extra other than 0.5 and 1 or 0\n",
    "taxi_sdf = taxi_sdf.filter(taxi_sdf[\"Extra\"].isin([0.5,1,0]))\n",
    "\n",
    "# remove any data with invalid RateCodeID\n",
    "taxi_sdf = taxi_sdf.filter(taxi_sdf[\"RateCodeID\"].isin([1,2,3,4,5,6]))\n",
    "\n",
    "# retain any record with fare amount less than total amount\n",
    "taxi_sdf = taxi_sdf.filter(taxi_sdf[\"Fare_amount\"] < taxi_sdf[\"Total_amount\"])\n",
    "\n",
    "# retain record with valid toll_amount\n",
    "taxi_sdf = taxi_sdf.filter(F.col('Tolls_amount') >= 0)\n",
    "taxi_sdf= taxi_sdf.filter(F.col('Tolls_amount') <=38)\n",
    "\n",
    "\n",
    "# change the casing for feature name \n",
    "filter_sdf = taxi_sdf.select('tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', \n",
    "                         'PULocationID', 'DOLocationID', 'payment_type', 'tip_amount', 'Fare_amount')\n",
    "for column_name, new_data_type in data_type_mappings.items():\n",
    "     filter_sdf = filter_sdf.withColumn(column_name, col(column_name).cast(new_data_type))\n",
    "\n",
    "consistent_col_casing = [F.col(col_name).alias(col_name.lower()) for col_name in filter_sdf.columns]\n",
    "filter_sdf = filter_sdf.select(*consistent_col_casing)\n",
    "\n",
    "# focusing on manhattan data\n",
    "taxi_sdf = taxi_sdf.join(location, location['LocationID']==taxi_sdf[\"pulocationid\"], \"inner\")\n",
    "taxi_sdf = taxi_sdf.drop('LocationID')\n",
    "taxi_sdf = taxi_sdf.filter(F.col(\"Borough\") == 'Manhattan')\n",
    "\n",
    "\n",
    "output_folder = \"../data/raw/test_taxi_2019/03\"  # Replace with the desired output folder\n",
    "filter_sdf.write.parquet(output_folder, mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad958d0-3b14-4a25-b0ac-d365a5ad7434",
   "metadata": {},
   "source": [
    "### filtering and cleaning to curated data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92db4c8c-ab22-4925-a323-d64e39429766",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# read data from raw\n",
    "temp_sdf = spark.read.parquet('../data/raw/test_taxi_2019/*')\n",
    "\n",
    "# remove any record with invalide passenger counts\n",
    "# valid passenger count are between 1-7\n",
    "temp_sdf = temp_sdf.where((F.col('passenger_count') > 0) & (F.col('passenger_count')<=7 ))\n",
    "\n",
    "# remove any record with less than 3 dollar fare_amount and more than $120\n",
    "temp_sdf = temp_sdf.filter(F.col('fare_amount') > 3)\n",
    "temp_sdf = temp_sdf.filter(F.col('fare_amount') <120)\n",
    "\n",
    "\n",
    "# remove any record with negative tipping and over $100 tip\n",
    "temp_sdf = temp_sdf.filter(F.col('tip_amount') >= 0)\n",
    "temp_sdf = temp_sdf.filter(F.col('tip_amount') <= 100)\n",
    "\n",
    "#remove any instance with trip distance less than 0.2 mile \n",
    "temp_sdf = temp_sdf.filter(F.col('trip_distance') >= 0.2)    \n",
    "\n",
    "#Define valid payment types\n",
    "valid_payment_types = [1, 2, 3, 4, 5, 6]\n",
    "# Remove rows with invalid payment types\n",
    "temp_sdf = temp_sdf.filter(temp_sdf[\"payment_type\"].isin(valid_payment_types))\n",
    "\n",
    "#Define valid location id\n",
    "location_ids = list(range(1,264))\n",
    "# Remove rows with invalid pick up location id\n",
    "# as we are mostly interest in trips that begin inside new york\n",
    "temp_sdf = temp_sdf.filter(temp_sdf[\"pulocationid\"].isin(location_ids))\n",
    "\n",
    "#filter out records where payment_type is not equal to 1 but tip_amount is greater than 0\n",
    "# as record with payment_type!=1 and tip_amount >0 violates the bussiness rule \n",
    "### remove negative tip\n",
    "temp_sdf = temp_sdf.filter((F.col(\"payment_type\") == 1) | (F.col(\"tip_amount\") <= 0))\n",
    "\n",
    "#filter out all record with non credicard payments \n",
    "temp_sdf = temp_sdf.filter((F.col(\"payment_type\") == 1))\n",
    "\n",
    "# Create new feature trip_duration to calculate trip duration in minute\n",
    "temp_sdf = temp_sdf.withColumn(\"trip_duration_minutes\", round((unix_timestamp(col(\"tpep_dropoff_datetime\")).cast(\"double\") - unix_timestamp(col(\"tpep_pickup_datetime\")).cast(\"double\"))/ 60, 3))\n",
    "\n",
    "# remove any trip/instance with trip duration less than a minute and longer than 2.5 hours \n",
    "temp_sdf = temp_sdf.filter(F.col('trip_duration_minutes') >= 1)\n",
    "temp_sdf = temp_sdf.filter(F.col('trip_duration_minutes') <= 150)\n",
    "\n",
    "\n",
    "# Create new feature of pickup_day and hour (the assitant feature) from pickup_datetime attribute\n",
    "temp_sdf = temp_sdf.withColumn(\"pickup_day\", date_format(col(\"tpep_pickup_datetime\"), \"yyyy-MM-dd\"))\n",
    "temp_sdf = temp_sdf.withColumn(\"hour\", date_format(col(\"tpep_pickup_datetime\"), \"HH\").cast(\"int\"))\n",
    "\n",
    "#### Remove any record that is not in our research time period \n",
    "# Define the start and end dates of the desired period\n",
    "start_date = datetime.strptime(\"2019-03-01\", \"%Y-%m-%d\")\n",
    "end_date = datetime.strptime(\"2019-03-31\", \"%Y-%m-%d\")\n",
    "# Filter out records not within the desired period\n",
    "temp_sdf = temp_sdf.filter((col(\"pickup_day\") >= start_date) & (col(\"pickup_day\") <= end_date))\n",
    "\n",
    "\n",
    "# Include new feature week number\n",
    "temp_sdf = temp_sdf.withColumn(\"week_number\", weekofyear(\"pickup_day\"))\n",
    "\n",
    "\n",
    "\n",
    "# Drop the tpep_pickup_datetime, tpep_dropoff_datetime, and other temporary/assistant features like hour and what_day, since we no longer using them\n",
    "temp_sdf = temp_sdf.drop(\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"payment_type\", \"dolocationid\")\n",
    "\n",
    "# dropping any row/instance of data contain missing value\n",
    "temp_sdf = temp_sdf.na.drop()\n",
    "\n",
    "output_folder = \"../data/curated/test_taxi_data/\"  # Replace with the desired output folder\n",
    "temp_sdf.write.parquet(output_folder, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f26fe71-2a38-4b79-b772-2641ab811e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3443252"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_taxi_sdf = spark.read.parquet('../data/curated/test_taxi_data/*')\n",
    "test_taxi_sdf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33506b99-5305-46d3-99a5-f80af9952c8f",
   "metadata": {},
   "source": [
    "## Now preprocess the weather test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66ef6eca-9e8b-40f7-87fd-19a4802d7b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_sdf = spark.read.csv(\"../data/landing/test_weather_data/test_weather_data.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5b28608-3596-495c-9283-3268705d177a",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep =  [\"datetime\", \"temp\", 'feelslike', \"snow\",\"windspeed\", 'cloudcover', \"humidity\", \"sealevelpressure\", \"conditions\"]\n",
    "weather_sdf = weather_sdf.select(*columns_to_keep)\n",
    "\n",
    "# save to raw \n",
    "output_folder = \"../data/raw/test_weather_data/\"  # Replace with the desired output folder\n",
    "weather_sdf.write.parquet(output_folder, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4071bb4-48ef-4647-8fa0-b8ee93a783d8",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fc5efc1-20e6-4d2b-b496-f83ecc924a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.read.parquet(\"../data/raw/test_weather_data/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9980b78f-d864-4e72-acf1-37eb12bb9db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# snow depth cannot be negative \n",
    "sdf = sdf.filter(F.col(\"snow\") >= 0) \n",
    "# wind speed cannot be negative \n",
    "sdf = sdf.filter(F.col(\"windspeed\") >= 0) \n",
    "# assume temperate of a day in new york in 2018 does not \n",
    "# exceed the maximum temperature and minimum temperature record in US\n",
    "sdf = sdf.filter(F.col(\"temp\") < 56) \n",
    "sdf = sdf.filter(F.col(\"temp\") > -62)\n",
    "#remove any record with null \n",
    "sdf = sdf.na.drop()\n",
    "\n",
    "# create a new feature to store the hour time of the day\n",
    "sdf = sdf.withColumn(\"hour_of_the_day\", hour(col(\"datetime\")))\n",
    "sdf = sdf.withColumn(\"date\", date_format(col(\"datetime\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# Extract day of the week from pickup_time (Like Monday, Tuesday.....etc)\n",
    "sdf = sdf.withColumn(\"what_day\", date_format(col(\"date\"), \"EEEE\"))\n",
    "# Create a new column for weekday or weekend from what day \n",
    "sdf = sdf.withColumn(\"day_of_the_week\", when(col(\"what_day\").isin(\"Saturday\", \"Sunday\"), \"Weekend\").otherwise(\"Weekday\"))\n",
    "\n",
    "# Drop the original \"datetime\" column as we will focuse on hourly analysi\n",
    "sdf = sdf.drop(\"datetime\", \"what_day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b93044b1-71dc-4007-a7b1-a77e716dfb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to curated \n",
    "output= \"../data/curated/test_weather_data/\"  # Replace with the desired output folder\n",
    "sdf.write.parquet(output, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac9af84-fd65-409c-8aac-40226bddb162",
   "metadata": {},
   "source": [
    "## Now merge the two data to form our test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "369c18e4-2ffa-4ff1-a433-44a1b8115cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_test = spark.read.parquet('../data/curated/test_taxi_data/*')\n",
    "weather = spark.read.parquet('../data/curated/test_weather_data/*')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f1b15e-f12e-4419-b701-298103776209",
   "metadata": {},
   "source": [
    "#### group the taxi test based one the date and hourly time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "777613eb-bf98-42a7-9a44-1a8ba9404483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by the hour of the day and calculate the average trip duration and tip amount\n",
    "gb_taxi = taxi_test.groupBy('pickup_day',\"hour\").agg(\n",
    "    F.count(\"*\").alias(\"trip_count\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dd8331-c5a0-4005-bacf-fc675d393157",
   "metadata": {},
   "source": [
    "### join with weather "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf0d1424-4085-4880-9ffd-a5dec92374e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the DataFrames using pickup_date and formatted_datetime columns\n",
    "# also rename to sample for convenient \n",
    "test_sdf = gb_taxi.join(weather, (gb_taxi[\"pickup_day\"] == weather[\"date\"]) & (gb_taxi[\"hour\"] == weather[\"hour_of_the_day\"]), \"inner\")\n",
    "\n",
    "\n",
    "test_sdf = test_sdf.withColumn('log(windspeed)', log(test_sdf['windspeed']+1)) # add 1 to aviod zero windspeed\n",
    "\n",
    "# drop the duplictae coloumn\n",
    "test_sdf = test_sdf.drop(\"windspeed\", \"pickup_day\", \"hour_of_the_day\", 'snow', 'feelslike', 'sealevelpressure','date', 'cloudcover')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e0f569-6a3a-4a02-9f90-98420b38176f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d68a145b-37c9-453a-b86d-1b446a15e3b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "719"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sdf.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60fa450-1ebb-46db-9163-00ea3833fe03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91b70edd-0458-4881-842d-72985e9f8f84",
   "metadata": {},
   "source": [
    "#### save the test sample to curated file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a54c451-91d7-4d93-b5be-82e4778fa746",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# save to curated \n",
    "output_folder = \"../data/curated/test_data/\"  # Replace with the desired output folder\n",
    "test_sdf.write.parquet(output_folder, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5c9a09-abce-45cb-a799-4beb398e72bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f1f8db-6c66-4776-9404-5d9e2cb3e42a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3664e8d5-3ab7-465c-9cd0-ae5a8c1c1031",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
